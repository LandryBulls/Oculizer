# v4 Scene Predictor - MFCC-Augmented Embeddings

## Overview

The v4 scene predictor combines EfficientAT embeddings with MFCC features for improved scene classification:
- **EfficientAT embeddings**: 1920 dimensions (audio neural network features)
- **MFCC features**: 128 dimensions (mel-frequency cepstral coefficients)
- **Total feature vector**: 2048 dimensions

## Pipeline

1. **Feature Extraction**: EfficientAT embedding (1920-dim) + MFCC mean (128-dim) = 2048-dim
2. **Scaling**: StandardScaler normalization
3. **Dimensionality Reduction**: PCA retaining 95% variance
4. **Clustering**: KMeans with 100 clusters
5. **Mapping**: Cluster ID → Scene name

## Files Created

### Training Scripts (in `notebooks/`)
- `create_v4_embeddings.py` - Augments existing EfficientAT embeddings with MFCCs
- `create_v4_clusters.py` - Runs clustering pipeline and generates models

### Predictor (in `oculizer/scene_predictors/v4/`)
- `predictor.py` - Real-time scene prediction with MFCC+embedding features
- `__init__.py` - Module initialization

### Models (generated by scripts, saved to `v4/`)
- `scaler.pkl` - StandardScaler for feature normalization
- `pca_95.pkl` - PCA model (95% variance retained)
- `kmeans_100.pkl` - KMeans clustering model (100 clusters)
- `scene_mapping.json` - Cluster ID to scene name mapping (needs manual filling)

### Data Files (generated by scripts)
- `notebooks/outputs/efficientat_mfcc_embeddings.pkl` - Augmented embeddings
- `notebooks/outputs/v4_clustersamples/cluster_*.wav` - Audio samples for each cluster

## Setup Instructions

### Step 1: Generate Augmented Embeddings

Run the embeddings script to add MFCCs to existing EfficientAT embeddings:

```bash
cd notebooks
python create_v4_embeddings.py
```

This will:
- Load existing embeddings from `outputs/efficientat_song_embeddings.pkl`
- Extract 128 MFCCs for each audio chunk
- Concatenate MFCCs to embeddings (1920 + 128 = 2048 dims)
- Save to `outputs/efficientat_mfcc_embeddings.pkl`

**Requirements**: librosa, numpy, joblib, tqdm

### Step 2: Generate Clustering Models

Run the clustering script to create the v4 models:

```bash
cd notebooks
python create_v4_clusters.py
```

This will:
- Load augmented embeddings
- Run Scale → PCA (95% variance) → KMeans (100 clusters) pipeline
- Save models to `oculizer/scene_predictors/v4/`
- Generate cluster audio samples in `outputs/v4_clustersamples/`
- Create empty `scene_mapping.json` template

**Requirements**: sklearn, librosa, numpy, joblib, soundfile

### Step 3: Fill Scene Mapping

After generating the models, you need to manually map clusters to scenes:

1. Listen to cluster samples in `notebooks/outputs/v4_clustersamples/`
2. Edit `oculizer/scene_predictors/v4/scene_mapping.json`
3. Replace "placeholder" values with actual scene names from your `scenes/` directory

Example:
```json
{
  "0": "party",
  "1": "ambient1",
  "2": "disco",
  ...
}
```

### Step 4: Test the Predictor

Run Oculizer with the v4 predictor:

```bash
python oculize.py --predictor-version v4
```

Or with specific options:
```bash
python oculize.py --predictor-version v4 --profile garage --input-device scarlett
```

## Technical Details

### MFCC Extraction Parameters
- `n_mfcc`: 128
- `n_fft`: 2048
- `hop_length`: 512
- Aggregation: Mean across time dimension

### Clustering Parameters
- PCA variance retained: 95%
- Number of clusters: 100
- Random state: 0 (for reproducibility)
- Sample rate: 48000 Hz
- Chunk length: 4 seconds

### Feature Vector Composition
```
[EfficientAT embedding: 1920 dims] + [MFCC mean: 128 dims] = [Total: 2048 dims]
```

## Differences from v3

| Aspect | v3 | v4 |
|--------|----|----|
| Features | EfficientAT only (1920-dim) | EfficientAT + MFCC (2048-dim) |
| PCA Variance | 300 components (fixed) | 95% variance (adaptive) |
| Clusters | 120 | 100 |
| Pipeline | Scale → PCA → Cluster | Scale → PCA → Cluster |

## Usage in Code

```python
from oculizer.scene_predictors import get_predictor

# Get v4 predictor
ScenePredictor = get_predictor(version='v4')
predictor = ScenePredictor()

# Predict scene from audio chunk (4 seconds at 48000 Hz)
import numpy as np
audio_chunk = np.random.randn(4 * 48000)  # Your audio data here
scene_name = predictor.predict(audio_chunk)
print(f"Predicted scene: {scene_name}")

# Get cluster number as well
scene_name, cluster_id = predictor.predict(audio_chunk, return_cluster=True)
print(f"Scene: {scene_name}, Cluster: {cluster_id}")
```

## Troubleshooting

### Import Errors
Make sure all required packages are installed:
```bash
pip install librosa numpy joblib scikit-learn soundfile torch efficientat
```

### Missing Embeddings File
The v4 scripts require `efficientat_song_embeddings.pkl` to exist. Generate it first using:
```bash
cd notebooks
python extract_song_embeddings_efficientat.py
```

### CUDA/GPU Issues
If you encounter GPU memory issues, the predictor will automatically fall back to CPU.

### Scene Mapping Not Found
Make sure you've run `create_v4_clusters.py` to generate `scene_mapping.json`, then fill it in manually.

## Next Steps

1. ✅ Scripts created
2. ✅ Predictor implemented
3. ⏳ Run `create_v4_embeddings.py` (requires proper Python environment)
4. ⏳ Run `create_v4_clusters.py`
5. ⏳ Fill in `scene_mapping.json`
6. ⏳ Test with `oculize.py --predictor-version v4`

