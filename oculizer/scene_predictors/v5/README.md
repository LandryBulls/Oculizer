# v5 Scene Predictor - Enhanced Audio Features

## Overview

The v5 scene predictor uses comprehensive audio features for improved scene classification:
- **EfficientAT embeddings**: 1920 dimensions (neural network semantic features)
- **MFCC mean**: 128 dimensions (spectral envelope)
- **MFCC std**: 128 dimensions (temporal dynamics)
- **Spectral features**: 12 dimensions (brightness, energy, texture)
- **Total feature vector**: 2188 dimensions

## Feature Breakdown

### EfficientAT Embeddings (1920-dim)
High-level semantic features learned from audio data. Captures "what" is in the audio (e.g., vocals, drums, synths).

### MFCC Statistics (256-dim)
- **Mean (128-dim)**: Average spectral envelope - the "average timbre"
- **Std (128-dim)**: Temporal dynamics - "how much the timbre changes"

### Spectral Features (12-dim)
1. **Spectral Centroid** (1-dim): Brightness/center of mass of spectrum
   - Higher = brighter sound, lower = darker sound
   - Good for distinguishing color temperature in lighting

2. **Spectral Rolloff** (1-dim): Frequency below which 85% of energy is contained
   - Indicates "brightness" vs "fullness"

3. **Spectral Bandwidth** (1-dim): Width of spectral distribution
   - Narrow = pure tones, wide = noisy/rich textures

4. **Spectral Contrast** (7-dim): Peak-valley differences across 7 frequency bands
   - High contrast = punchy/percussive (strobes, flashes)
   - Low contrast = smooth/sustained (fades, washes)

5. **Zero Crossing Rate** (1-dim): How often signal crosses zero
   - High = noisy/percussive
   - Low = tonal/harmonic

6. **RMS Energy** (1-dim): Overall loudness
   - Direct mapping to light intensity

## Pipeline

1. **Feature Extraction**: 
   - EfficientAT embedding (1920-dim)
   - MFCC mean + std (256-dim)
   - Spectral features (12-dim)
   - Concatenate → 2188-dim vector

2. **Scaling**: StandardScaler normalization

3. **Dimensionality Reduction**: PCA retaining 95% variance

4. **Clustering**: KMeans with 100 clusters

5. **Mapping**: Cluster ID → Scene name

## Files

### Training Scripts (in `notebooks/`)
- `create_v5_embeddings.py` - Extract enhanced audio features and augment embeddings
- `create_v5_clusters.py` - Run clustering pipeline and generate models

### Predictor (in `oculizer/scene_predictors/v5/`)
- `predictor.py` - Real-time scene prediction with comprehensive features
- `__init__.py` - Module initialization

### Models (generated by scripts, saved to `v5/`)
- `scaler.pkl` - StandardScaler for feature normalization
- `pca_95.pkl` - PCA model (95% variance retained)
- `kmeans_100.pkl` - KMeans clustering model (100 clusters)
- `scene_mapping.json` - Cluster ID to scene name mapping (needs manual filling)

### Data Files (generated by scripts)
- `notebooks/outputs/efficientat_v5_embeddings.pkl` - Enhanced embeddings
- `notebooks/outputs/v5_clustersamples/cluster_*.wav` - Audio samples for each cluster

## Setup Instructions

### Step 1: Generate Enhanced Embeddings

```bash
cd notebooks
python create_v5_embeddings.py
```

This will:
- Load existing EfficientAT embeddings from `outputs/efficientat_song_embeddings.pkl`
- Extract comprehensive audio features for each chunk:
  - MFCC mean and std (256-dim)
  - Spectral features (12-dim)
- Concatenate with embeddings (1920 + 268 = 2188 dims)
- Save to `outputs/efficientat_v5_embeddings.pkl`

**Requirements**: librosa, numpy, joblib, tqdm

**Processing time**: ~15-45 minutes depending on dataset size

### Step 2: Generate Clustering Models

```bash
cd notebooks
python create_v5_clusters.py
```

This will:
- Load enhanced embeddings
- Run Scale → PCA (95% variance) → KMeans (100 clusters) pipeline
- Save models to `oculizer/scene_predictors/v5/`
- Generate 100 cluster audio samples in `outputs/v5_clustersamples/`
- Create empty `scene_mapping.json` template

**Requirements**: sklearn, librosa, numpy, joblib, soundfile

### Step 3: Fill Scene Mapping

1. Listen to cluster samples in `notebooks/outputs/v5_clustersamples/`
2. Edit `oculizer/scene_predictors/v5/scene_mapping.json`
3. Map each cluster (0-99) to appropriate scene names

Example:
```json
{
  "0": "party",
  "1": "ambient1",
  "2": "disco",
  ...
}
```

### Step 4: Test the Predictor

```bash
python oculize.py --predictor-version v5
```

Or with specific options:
```bash
python oculize.py --predictor-version v5 --profile garage --input-device scarlett
```

## Technical Details

### Audio Feature Parameters
- **MFCC**: n_mfcc=128, n_fft=2048, hop_length=512
- **Spectral features**: Same n_fft and hop_length
- **Aggregation**: Mean (and std for MFCCs) across time dimension
- **Sample rate**: 48000 Hz
- **Chunk length**: 4 seconds

### Clustering Parameters
- **PCA variance**: 95% (adaptive dimensionality)
- **Number of clusters**: 100
- **Random state**: 0 (reproducibility)

### Feature Vector Composition
```
[EfficientAT: 1920] + [MFCC mean: 128] + [MFCC std: 128] + [Spectral: 12] = [Total: 2188]
```

Breakdown of spectral features (12 dims):
- Centroid: 1
- Rolloff: 1
- Bandwidth: 1
- Contrast: 7 (one per frequency band)
- ZCR: 1
- RMS: 1

## Why v5 is Better than v4

### v4 Features
- EfficientAT: 1920-dim
- MFCC mean only: 128-dim
- **Total: 2048-dim**

### v5 Features
- EfficientAT: 1920-dim
- MFCC mean: 128-dim
- MFCC std: 128-dim ✨ (temporal dynamics)
- Spectral features: 12-dim ✨ (brightness, energy, texture)
- **Total: 2188-dim**

### Key Improvements

1. **MFCC std** captures how much the timbre changes over time
   - Static passages vs dynamic passages
   - Helps distinguish sustained notes from rapidly changing textures

2. **Spectral features** directly capture lighting-relevant properties:
   - **Brightness** (centroid, rolloff) → Color temperature
   - **Energy** (RMS) → Light intensity
   - **Texture** (contrast, bandwidth) → Strobe vs fade effects
   - **Percussiveness** (ZCR) → Sharp vs smooth transitions

3. **Better scene discrimination** for lighting control:
   - More accurate distinction between similar semantic content with different "feel"
   - Example: Both have vocals and drums, but one is bright/punchy (strobes) vs dark/smooth (washes)

## Comparison Table

| Feature | v3 | v4 | v5 |
|---------|----|----|-----|
| EfficientAT | 1920 | 1920 | 1920 |
| MFCC mean | - | 128 | 128 |
| MFCC std | - | - | 128 ✨ |
| Spectral features | - | - | 12 ✨ |
| **Total dims** | 1920 | 2048 | 2188 |
| PCA components | 300 (fixed) | 95% var | 95% var |
| Clusters | 120 | 100 | 100 |

## Usage in Code

```python
from oculizer.scene_predictors import get_predictor

# Get v5 predictor
ScenePredictor = get_predictor(version='v5')
predictor = ScenePredictor()

# Predict scene from audio chunk (4 seconds at 48000 Hz)
import numpy as np
audio_chunk = np.random.randn(4 * 48000)  # Your audio data here
scene_name = predictor.predict(audio_chunk)
print(f"Predicted scene: {scene_name}")

# Get cluster number as well
scene_name, cluster_id = predictor.predict(audio_chunk, return_cluster=True)
print(f"Scene: {scene_name}, Cluster: {cluster_id}")
```

## Lighting Use Cases

The spectral features make v5 particularly good at:

1. **Energy-based scenes**
   - RMS energy → Light intensity
   - Example: Quiet intro (dim) → Drop (bright)

2. **Brightness/color mapping**
   - Spectral centroid → Color temperature
   - Example: Dark bass (red/purple) → Bright synth (white/cyan)

3. **Texture-based effects**
   - Spectral contrast → Effect type
   - High contrast (percussive) → Strobes, flashes
   - Low contrast (smooth) → Fades, washes

4. **Dynamic vs static**
   - MFCC std → Scene consistency
   - High std (changing) → Cycling effects
   - Low std (steady) → Static scenes

## Troubleshooting

### Missing Dependencies
```bash
pip install librosa numpy joblib scikit-learn soundfile torch efficientat
```

### Slow Processing
Spectral feature extraction is computationally intensive. Expected time:
- ~15-30 minutes for 500 songs on modern CPU
- GPU acceleration helps with EfficientAT but not librosa features

### Memory Issues
The v5 embeddings file is larger (~2.2GB). Ensure sufficient disk space.

## Next Steps

1. ✅ Scripts created
2. ✅ Predictor implemented
3. ⏳ Run `create_v5_embeddings.py`
4. ⏳ Run `create_v5_clusters.py`
5. ⏳ Fill in `scene_mapping.json`
6. ⏳ Test with `oculize.py --predictor-version v5`

